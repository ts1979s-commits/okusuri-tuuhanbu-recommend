# RAGè¨­è¨ˆæ›¸ - å•†å“ãƒ¬ã‚³ãƒ¡ãƒ³ãƒ‰LLMã‚¢ãƒ—ãƒª

## ğŸ“‹ **RAGï¼ˆRetrieval-Augmented Generationï¼‰æ¦‚è¦**

### RAGã‚·ã‚¹ãƒ†ãƒ ã¨ã¯
æœ¬ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§ã¯ã€FAISSã‚’æ´»ç”¨ã—ãŸãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã«ã‚ˆã‚‹Retrieval-Augmented Generationã‚·ã‚¹ãƒ†ãƒ ã‚’å®Ÿè£…ã—ã¦ã„ã¾ã™ã€‚å¾“æ¥ã®æ–‡å­—åˆ—ãƒãƒƒãƒãƒ³ã‚°æ¤œç´¢ã«åŠ ãˆã¦ã€æ„å‘³çš„é¡ä¼¼åº¦ã«ã‚ˆã‚‹é«˜ç²¾åº¦ãªå•†å“æ¤œç´¢ã‚’æä¾›ã—ã¾ã™ã€‚

### å®Ÿè£…çŠ¶æ³
- âœ… **åŸºæœ¬æ¤œç´¢**: æ–‡å­—åˆ—ãƒãƒƒãƒãƒ³ã‚°æ¤œç´¢ï¼ˆãƒ¡ã‚¤ãƒ³æ©Ÿèƒ½ï¼‰
- âœ… **RAGæ¤œç´¢**: FAISSãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³æ©Ÿèƒ½ï¼‰
- âœ… **ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰**: ä¸¡æ¤œç´¢æ–¹å¼ã®ä½µç”¨å¯èƒ½

## ğŸ—ï¸ **RAGã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£è¨­è¨ˆ**

### ã‚·ã‚¹ãƒ†ãƒ æ§‹æˆå›³
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¯ã‚¨ãƒª   â”‚ -> â”‚   åŸ‹ã‚è¾¼ã¿ç”Ÿæˆ    â”‚ -> â”‚  ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢     â”‚
â”‚   "ç–²åŠ´å›å¾©"      â”‚    â”‚  OpenAI API     â”‚    â”‚  FAISS Index    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                      |
                                                      v
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   æ¤œç´¢çµæœè¡¨ç¤º    â”‚ <- â”‚   çµæœç”Ÿæˆå‡¦ç†   â”‚ <- â”‚  é¡ä¼¼åº¦ãƒ©ãƒ³ã‚­ãƒ³ã‚°  â”‚
â”‚  å•†å“ãƒªã‚¹ãƒˆè¡¨ç¤º   â”‚    â”‚  å•†å“æƒ…å ±å–å¾—    â”‚    â”‚  ä¸Šä½Kä»¶å–å¾—      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼
```
1. ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ› â†’ ãƒ†ã‚­ã‚¹ãƒˆæ­£è¦åŒ–
2. OpenAI Embeddings â†’ ãƒ™ã‚¯ãƒˆãƒ«å¤‰æ›
3. FAISS Index Search â†’ é¡ä¼¼ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢
4. çµæœãƒãƒƒãƒ”ãƒ³ã‚° â†’ å•†å“æƒ…å ±å–å¾—
5. ãƒ©ãƒ³ã‚­ãƒ³ã‚° â†’ é¡ä¼¼åº¦é †ã‚½ãƒ¼ãƒˆ
6. UIè¡¨ç¤º â†’ ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¸çµæœæç¤º
```

## ğŸ”§ **æŠ€è¡“å®Ÿè£…è©³ç´°**

### 1. FAISSRAGSystemã‚¯ãƒ©ã‚¹è¨­è¨ˆ

```python
class FAISSRAGSystem:
    def __init__(self, openai_api_key=None):
        """
        RAGã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
        
        Args:
            openai_api_key: OpenAI APIã‚­ãƒ¼
        """
        self.openai_api_key = openai_api_key
        self.index = None
        self.documents = []
        self.metadata = []
        self.embeddings_model = None
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        self.cache_paths = {
            'index': 'data/faiss_index.bin',
            'documents': 'data/documents.pkl',
            'metadata': 'data/metadata.pkl',
            'mapping': 'data/faiss_mapping.json'
        }
    
    def initialize_openai(self):
        """OpenAI EmbeddingsåˆæœŸåŒ–"""
        if self.openai_api_key:
            from openai import OpenAI
            self.client = OpenAI(api_key=self.openai_api_key)
            return True
        return False
    
    def build_index(self, products_df):
        """FAISSã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰"""
        documents = []
        metadata = []
        
        # å•†å“ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆç”Ÿæˆ
        for idx, row in products_df.iterrows():
            doc_text = self._create_document_text(row)
            documents.append(doc_text)
            metadata.append({
                'index': idx,
                'å•†å“å': row['å•†å“å'],
                'ã‚«ãƒ†ã‚´ãƒª': row['ã‚«ãƒ†ã‚´ãƒª']
            })
        
        # åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ç”Ÿæˆ
        embeddings = self._generate_embeddings(documents)
        
        # FAISSã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰
        import faiss
        dimension = len(embeddings[0])
        self.index = faiss.IndexFlatIP(dimension)  # å†…ç©é¡ä¼¼åº¦
        self.index.add(np.array(embeddings).astype('float32'))
        
        self.documents = documents
        self.metadata = metadata
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜
        self._save_cache()
        
        return True
```

### 2. åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ç”Ÿæˆ

```python
def _create_document_text(self, row):
    """
    å•†å“ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ
    
    Args:
        row: å•†å“ãƒ‡ãƒ¼ã‚¿è¡Œï¼ˆpandas Seriesï¼‰
    
    Returns:
        str: æ§‹é€ åŒ–ã•ã‚ŒãŸãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒ†ã‚­ã‚¹ãƒˆ
    """
    doc_parts = []
    
    # å•†å“åï¼ˆé‡è¦åº¦: é«˜ï¼‰
    if pd.notna(row['å•†å“å']):
        doc_parts.append(f"å•†å“å: {row['å•†å“å']}")
    
    # åŠ¹æœï¼ˆé‡è¦åº¦: é«˜ï¼‰
    if pd.notna(row['åŠ¹æœ']):
        doc_parts.append(f"åŠ¹æœ: {row['åŠ¹æœ']}")
    
    # ã‚«ãƒ†ã‚´ãƒªï¼ˆé‡è¦åº¦: ä¸­ï¼‰
    if pd.notna(row['ã‚«ãƒ†ã‚´ãƒª']):
        doc_parts.append(f"ã‚«ãƒ†ã‚´ãƒª: {row['ã‚«ãƒ†ã‚´ãƒª']}")
    
    # æˆåˆ†ï¼ˆé‡è¦åº¦: ä¸­ï¼‰
    if pd.notna(row['æˆåˆ†']):
        doc_parts.append(f"æˆåˆ†: {row['æˆåˆ†']}")
    
    # èª¬æ˜ï¼ˆé‡è¦åº¦: ä½ï¼‰
    if pd.notna(row['èª¬æ˜']):
        doc_parts.append(f"èª¬æ˜: {row['èª¬æ˜']}")
    
    return "\n".join(doc_parts)

def _generate_embeddings(self, texts):
    """
    ãƒ†ã‚­ã‚¹ãƒˆãƒªã‚¹ãƒˆã‹ã‚‰åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ç”Ÿæˆ
    
    Args:
        texts: ãƒ†ã‚­ã‚¹ãƒˆã®ãƒªã‚¹ãƒˆ
    
    Returns:
        List[List[float]]: åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã®ãƒªã‚¹ãƒˆ
    """
    embeddings = []
    
    for text in texts:
        try:
            response = self.client.embeddings.create(
                input=text,
                model="text-embedding-3-small"  # é«˜é€Ÿãƒ»è»½é‡ãƒ¢ãƒ‡ãƒ«
            )
            embedding = response.data[0].embedding
            embeddings.append(embedding)
            
        except Exception as e:
            print(f"åŸ‹ã‚è¾¼ã¿ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ã‚¼ãƒ­ãƒ™ã‚¯ãƒˆãƒ«
            embeddings.append([0.0] * 1536)
    
    return embeddings
```

### 3. æ¤œç´¢å®Ÿè¡Œæ©Ÿèƒ½

```python
def search(self, query, k=5):
    """
    ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢å®Ÿè¡Œ
    
    Args:
        query: æ¤œç´¢ã‚¯ã‚¨ãƒªæ–‡å­—åˆ—
        k: å–å¾—ä»¶æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 5ï¼‰
    
    Returns:
        List[Dict]: æ¤œç´¢çµæœãƒªã‚¹ãƒˆ
    """
    if not self.index or not self.openai_api_key:
        return []
    
    try:
        # ã‚¯ã‚¨ãƒªã®åŸ‹ã‚è¾¼ã¿ç”Ÿæˆ
        query_embedding = self._generate_embeddings([query])[0]
        query_vector = np.array([query_embedding]).astype('float32')
        
        # FAISSæ¤œç´¢å®Ÿè¡Œ
        scores, indices = self.index.search(query_vector, k)
        
        # çµæœæ§‹ç¯‰
        results = []
        for i, (score, idx) in enumerate(zip(scores[0], indices[0])):
            if idx < len(self.metadata):
                result = {
                    'rank': i + 1,
                    'score': float(score),
                    'document': self.documents[idx],
                    'metadata': self.metadata[idx]
                }
                results.append(result)
        
        return results
        
    except Exception as e:
        print(f"RAGæ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
        return []
```

## ğŸ—„ï¸ **ãƒ‡ãƒ¼ã‚¿æ§‹é€ è¨­è¨ˆ**

### ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ§‹é€ 
```python
# å•†å“ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆä¾‹
document_structure = {
    "text": """
å•†å“å: ã‚¹ãƒšãƒãƒ³
åŠ¹æœ: ç²¾åŠ›å¢—å¼·ã€æ»‹é¤Šå¼·å£®ã€ç–²åŠ´å›å¾©
ã‚«ãƒ†ã‚´ãƒª: ç”·æ€§å‘ã‘ã‚µãƒ—ãƒª
æˆåˆ†: é«˜éº—äººå‚ã€ãƒã‚«ã€äºœé‰›ã€ã‚¢ãƒ«ã‚®ãƒ‹ãƒ³
èª¬æ˜: å¤©ç„¶æˆåˆ†ã‚’é…åˆã—ãŸç”·æ€§å‘ã‘ã®æ»‹é¤Šå¼·å£®ã‚µãƒ—ãƒªãƒ¡ãƒ³ãƒˆ
""",
    "embedding": [0.123, -0.456, 0.789, ...],  # 1536æ¬¡å…ƒãƒ™ã‚¯ãƒˆãƒ«
    "metadata": {
        "index": 0,
        "å•†å“å": "ã‚¹ãƒšãƒãƒ³",
        "ã‚«ãƒ†ã‚´ãƒª": "ç”·æ€§å‘ã‘ã‚µãƒ—ãƒª"
    }
}
```

### FAISSã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹è¨­è¨ˆ
```python
# ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹è¨­å®š
index_config = {
    "type": "IndexFlatIP",        # å†…ç©é¡ä¼¼åº¦ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
    "dimension": 1536,            # OpenAI text-embedding-3-small
    "metric": "inner_product",    # å†…ç©é¡ä¼¼åº¦æ¸¬å®š
    "size": "35_products",        # 35å•†å“åˆ†ã®ãƒ™ã‚¯ãƒˆãƒ«
    "memory_usage": "~200MB"      # æ¨å®šãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡
}

# ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«æ§‹æˆ
cache_files = {
    "faiss_index.bin": "FAISSã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æœ¬ä½“",
    "documents.pkl": "å…ƒæ–‡æ›¸ãƒ†ã‚­ã‚¹ãƒˆ",
    "metadata.pkl": "å•†å“ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿", 
    "faiss_mapping.json": "è¨­å®šæƒ…å ±"
}
```

## ğŸ” **æ¤œç´¢ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ **

### é¡ä¼¼åº¦è¨ˆç®—æ‰‹æ³•
```python
# 1. ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ï¼ˆæ­£è¦åŒ–æ¸ˆã¿ï¼‰
def cosine_similarity(vec1, vec2):
    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))

# 2. å†…ç©é¡ä¼¼åº¦ï¼ˆFAISSä½¿ç”¨ï¼‰
def inner_product_similarity(query_vec, doc_vecs):
    return np.dot(query_vec, doc_vecs.T)

# 3. ãƒ¦ãƒ¼ã‚¯ãƒªãƒƒãƒ‰è·é›¢ï¼ˆè·é›¢ãƒ™ãƒ¼ã‚¹ï¼‰
def euclidean_distance(vec1, vec2):
    return np.linalg.norm(vec1 - vec2)
```

### ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 
```python
def rank_results(search_results, query):
    """
    æ¤œç´¢çµæœã®ãƒ©ãƒ³ã‚­ãƒ³ã‚°èª¿æ•´
    
    Args:
        search_results: FAISSæ¤œç´¢çµæœ
        query: å…ƒã‚¯ã‚¨ãƒª
    
    Returns:
        List: èª¿æ•´æ¸ˆã¿çµæœãƒªã‚¹ãƒˆ
    """
    ranked_results = []
    
    for result in search_results:
        # åŸºæœ¬ã‚¹ã‚³ã‚¢ï¼ˆFAISSé¡ä¼¼åº¦ï¼‰
        base_score = result['score']
        
        # ãƒ–ãƒ¼ã‚¹ãƒˆãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼é©ç”¨
        boosted_score = base_score
        
        # å•†å“åå®Œå…¨ä¸€è‡´ãƒ–ãƒ¼ã‚¹ãƒˆ
        if query.lower() in result['metadata']['å•†å“å'].lower():
            boosted_score *= 1.5
        
        # ã‚«ãƒ†ã‚´ãƒªä¸€è‡´ãƒ–ãƒ¼ã‚¹ãƒˆ  
        if query.lower() in result['metadata']['ã‚«ãƒ†ã‚´ãƒª'].lower():
            boosted_score *= 1.2
        
        result['final_score'] = boosted_score
        ranked_results.append(result)
    
    # æœ€çµ‚ã‚¹ã‚³ã‚¢é †ã§ã‚½ãƒ¼ãƒˆ
    return sorted(ranked_results, key=lambda x: x['final_score'], reverse=True)
```

## âš¡ **ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–**

### ã‚­ãƒ£ãƒƒã‚·ãƒ¥æˆ¦ç•¥
```python
class CacheManager:
    def __init__(self):
        self.embedding_cache = {}  # ã‚¯ã‚¨ãƒªåŸ‹ã‚è¾¼ã¿ã‚­ãƒ£ãƒƒã‚·ãƒ¥
        self.result_cache = {}     # æ¤œç´¢çµæœã‚­ãƒ£ãƒƒã‚·ãƒ¥
        self.max_cache_size = 1000
    
    def get_cached_embedding(self, text):
        """åŸ‹ã‚è¾¼ã¿ã‚­ãƒ£ãƒƒã‚·ãƒ¥å–å¾—"""
        return self.embedding_cache.get(text)
    
    def cache_embedding(self, text, embedding):
        """åŸ‹ã‚è¾¼ã¿ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜"""
        if len(self.embedding_cache) < self.max_cache_size:
            self.embedding_cache[text] = embedding
    
    def get_cached_results(self, query_hash):
        """æ¤œç´¢çµæœã‚­ãƒ£ãƒƒã‚·ãƒ¥å–å¾—"""
        return self.result_cache.get(query_hash)
```

### ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æœ€é©åŒ–
```python
def optimize_index():
    """FAISSã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æœ€é©åŒ–"""
    # PCAæ¬¡å…ƒå‰Šæ¸›ï¼ˆãƒ¡ãƒ¢ãƒªå‰Šæ¸›ï¼‰
    pca_dimension = 512  # 1536 -> 512æ¬¡å…ƒ
    
    # é‡å­åŒ–ï¼ˆç²¾åº¦ vs é€Ÿåº¦ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ï¼‰
    quantizer = faiss.IndexFlatIP(pca_dimension)
    index = faiss.IndexIVFPQ(quantizer, pca_dimension, 8, 8, 8)
    
    return index
```

## ğŸ“Š **è©•ä¾¡ãƒ»ãƒ¡ãƒˆãƒªã‚¯ã‚¹**

### æ¤œç´¢ç²¾åº¦è©•ä¾¡
```python
evaluation_metrics = {
    # ç²¾åº¦æŒ‡æ¨™
    "precision_at_k": "ä¸Šä½Kä»¶ä¸­ã®é–¢é€£å•†å“å‰²åˆ",
    "recall_at_k": "é–¢é€£å•†å“ã®æ¤œç´¢ãƒ’ãƒƒãƒˆç‡", 
    "mrr": "Mean Reciprocal Rank",
    "ndcg": "æ­£è¦åŒ–å‰²å¼•ç´¯ç©åˆ©å¾—",
    
    # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŒ‡æ¨™
    "response_time": "å¹³å‡å¿œç­”æ™‚é–“",
    "throughput": "1ç§’ã‚ãŸã‚Šå‡¦ç†ã‚¯ã‚¨ãƒªæ•°",
    "memory_usage": "ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡",
    "index_size": "ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚µã‚¤ã‚º"
}
```

### ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹è¨­è¨ˆ
```python
test_cases = [
    # ç—‡çŠ¶ãƒ™ãƒ¼ã‚¹æ¤œç´¢
    {
        "query": "é ­ç—›",
        "expected": ["è§£ç†±é®ç—›è–¬", "é¢¨é‚ªè–¬"],
        "category": "symptom_search"
    },
    
    # æˆåˆ†ãƒ™ãƒ¼ã‚¹æ¤œç´¢
    {
        "query": "é«˜éº—äººå‚",
        "expected": ["ã‚¹ãƒšãƒãƒ³", "ãã®ä»–é«˜éº—äººå‚å•†å“"],
        "category": "ingredient_search"
    },
    
    # åŠ¹æœãƒ™ãƒ¼ã‚¹æ¤œç´¢
    {
        "query": "ç²¾åŠ›å¢—å¼·",
        "expected": ["ç”·æ€§å‘ã‘ã‚µãƒ—ãƒª"],
        "category": "effect_search"
    }
]
```

## ğŸ”§ **è¨­å®šãƒ»èª¿æ•´ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿**

### æ¤œç´¢ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
```python
search_config = {
    # æ¤œç´¢çµæœæ•°
    "top_k": 5,                    # ä¸Šä½5ä»¶å–å¾—
    "min_score_threshold": 0.7,    # æœ€ä½é¡ä¼¼åº¦é–¾å€¤
    
    # åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«
    "embedding_model": "text-embedding-3-small",
    "embedding_dimension": 1536,
    
    # FAISSè¨­å®š
    "index_type": "IndexFlatIP",
    "metric_type": "inner_product",
    
    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥è¨­å®š
    "enable_cache": True,
    "cache_ttl": 3600,             # 1æ™‚é–“
    "max_cache_entries": 1000
}
```

### ãƒ–ãƒ¼ã‚¹ãƒˆãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼
```python
boost_factors = {
    "exact_product_match": 1.5,    # å•†å“åå®Œå…¨ä¸€è‡´
    "category_match": 1.2,         # ã‚«ãƒ†ã‚´ãƒªä¸€è‡´
    "ingredient_match": 1.1,       # æˆåˆ†ä¸€è‡´
    "recent_product": 1.05,        # æ–°å•†å“ãƒ–ãƒ¼ã‚¹ãƒˆ
    "popular_product": 1.03        # äººæ°—å•†å“ãƒ–ãƒ¼ã‚¹ãƒˆ
}
```

## ğŸ›¡ï¸ **ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ãƒ»ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯**

### ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æˆ¦ç•¥
```python
def search_with_fallback(query, k=5):
    """
    RAGæ¤œç´¢ + ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ¤œç´¢
    
    1. RAGæ¤œç´¢å®Ÿè¡Œ
    2. å¤±æ•—æ™‚ â†’ åŸºæœ¬æ¤œç´¢ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
    3. çµæœãƒãƒ¼ã‚¸ãƒ»é‡è¤‡é™¤å»
    """
    try:
        # RAGæ¤œç´¢å®Ÿè¡Œ
        rag_results = rag_system.search(query, k)
        
        if rag_results:
            return rag_results
        
    except Exception as e:
        print(f"RAGæ¤œç´¢å¤±æ•—: {e}")
    
    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: åŸºæœ¬æ¤œç´¢
    print("åŸºæœ¬æ¤œç´¢ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯")
    return basic_search(query, "ç—‡çŠ¶")

def hybrid_search(query, k=5):
    """
    ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢ï¼ˆRAG + åŸºæœ¬æ¤œç´¢ï¼‰
    
    ä¸¡æ–¹ã®çµæœã‚’çµ±åˆã—ã¦æœ€é©ãªçµæœã‚’æä¾›
    """
    rag_results = rag_system.search(query, k)
    basic_results = basic_search(query, "ç—‡çŠ¶")
    
    # çµæœçµ±åˆãƒ»é‡è¤‡é™¤å»
    combined_results = merge_and_deduplicate(rag_results, basic_results)
    
    return combined_results[:k]
```

### ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
```python
class RAGException(Exception):
    """RAGã‚·ã‚¹ãƒ†ãƒ å°‚ç”¨ä¾‹å¤–"""
    pass

def robust_rag_search(query):
    """å …ç‰¢ãªRAGæ¤œç´¢å®Ÿè£…"""
    try:
        # å…¥åŠ›æ¤œè¨¼
        if not query or len(query.strip()) == 0:
            raise RAGException("ç©ºã®ã‚¯ã‚¨ãƒªã§ã™")
        
        # APIåˆ¶é™ãƒã‚§ãƒƒã‚¯
        if not check_api_limits():
            raise RAGException("APIåˆ¶é™ã«é”ã—ã¾ã—ãŸ")
        
        # æ¤œç´¢å®Ÿè¡Œ
        results = rag_system.search(query)
        
        # çµæœæ¤œè¨¼
        if not results:
            raise RAGException("æ¤œç´¢çµæœãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        
        return results
        
    except RAGException as e:
        print(f"RAGã‚¨ãƒ©ãƒ¼: {e}")
        return fallback_search(query)
    
    except Exception as e:
        print(f"äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {e}")
        return []
```

## ğŸš€ **ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ»é‹ç”¨è¨­å®š**

### ç’°å¢ƒåˆ¥è¨­å®š
```python
# é–‹ç™ºç’°å¢ƒ
dev_config = {
    "use_rag": True,
    "cache_enabled": False,
    "debug_logging": True,
    "api_timeout": 30
}

# æœ¬ç•ªç’°å¢ƒ  
prod_config = {
    "use_rag": True,
    "cache_enabled": True,
    "debug_logging": False,
    "api_timeout": 10
}
```

### ç›£è¦–ãƒ»ãƒ­ã‚°
```python
import logging

# RAGã‚·ã‚¹ãƒ†ãƒ å°‚ç”¨ãƒ­ã‚°
rag_logger = logging.getLogger('rag_system')
rag_logger.setLevel(logging.INFO)

def log_search_metrics(query, results, execution_time):
    """æ¤œç´¢ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®ãƒ­ã‚°è¨˜éŒ²"""
    rag_logger.info(f"æ¤œç´¢å®Ÿè¡Œ: query='{query}', "
                   f"results={len(results)}, "
                   f"time={execution_time:.3f}s")
```

## ğŸ“ˆ **ä»Šå¾Œã®æ”¹å–„è¨ˆç”»**

### Phase 1: ç²¾åº¦å‘ä¸Š
- ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«
- ã‚«ã‚¹ã‚¿ãƒ é¡ä¼¼åº¦è¨ˆç®—æ‰‹æ³•
- ãƒ‰ãƒ¡ã‚¤ãƒ³ç‰¹åŒ–è¾æ›¸

### Phase 2: ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°  
- åˆ†æ•£FAISSã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
- Redisçµæœã‚­ãƒ£ãƒƒã‚·ãƒ¥
- ãƒ­ãƒ¼ãƒ‰ãƒãƒ©ãƒ³ã‚µå¯¾å¿œ

### Phase 3: é«˜åº¦æ©Ÿèƒ½
- å¤šæ®µéšæ¤œç´¢ï¼ˆcoarse-to-fineï¼‰
- ãƒªãƒ©ãƒ³ã‚­ãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«
- ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯å­¦ç¿’

**RAGã‚·ã‚¹ãƒ†ãƒ è¨­è¨ˆå®Œäº†**: 2025å¹´11æœˆ11æ—¥